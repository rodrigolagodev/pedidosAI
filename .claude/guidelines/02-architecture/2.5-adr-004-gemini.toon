metadata:
  id: arch-adr-004
  version: 1.0
  lastUpdated: 2025-11-18
  references[2]: proj-stack,proj-overview

content:
  title: "ADR-004: Gemini 1.5 Flash for Order Parsing"
  purpose: Document the decision to use Gemini for parsing transcribed orders into structured data.

  sections[1]:
    - id: arch-adr-004-decision
      title: Decision Record
      status: Accepted
      date: 2025-11-18

      context: |
        Need an LLM to parse natural language orders into structured data (product, quantity, unit).
        Must handle Spanish culinary vocabulary and regional variations.

      decision: Use Gemini 1.5 Flash for parsing transcribed text into structured order items.

      rationale[5]:
        - Cost-effective (significantly cheaper than GPT-4)
        - Fast inference suitable for interactive use
        - Good Spanish language understanding
        - Large context window for complex prompts
        - Reliable JSON output with proper prompting

      consequences:
        positive[4]:
          - Low cost per parse (~$0.001 per order)
          - Sub-second response times
          - Good accuracy for structured extraction
          - Easy to iterate on prompts
        negative[3]:
          - Less capable than GPT-4 for complex reasoning
          - Requires careful prompt engineering
          - May need fallback for edge cases

      alternatives[3]:
        - id: arch-adr-004-alt-gpt4
          name: GPT-4
          rejected: true
          reason: "10x more expensive, slower, overkill for parsing task"
        - id: arch-adr-004-alt-claude
          name: Claude
          rejected: true
          reason: "Higher cost, similar capabilities for this task"
        - id: arch-adr-004-alt-local
          name: Local model (Llama)
          rejected: true
          reason: "Infrastructure complexity, slower, less accurate for Spanish"

      configuration:
        model: gemini-1.5-flash
        temperature: 0.1
        maxOutputTokens: 2048
        topP: 0.8

      pattern:
        description: Parsing prompt structure
        code: |
          const prompt = `Eres un asistente de pedidos de restaurante.

          Extrae los items del siguiente texto:
          "${transcription}"

          Responde SOLO con JSON válido:
          {
            "items": [
              {
                "product": "nombre normalizado",
                "quantity": número,
                "unit": "kg|units|dozen|...",
                "category": "meats|dairy|...",
                "confidence": 0.0-1.0
              }
            ]
          }

          Normaliza cantidades: "media docena" = 6, "kilo y medio" = 1.5`

          const result = await gemini.generateContent(prompt)
      antiPattern:
        description: Inefficient prompting patterns
        code: |
          // DON'T use high temperature for parsing
          const config = { temperature: 0.9 } // Too random!

          // DON'T ask for explanations when just need data
          const prompt = `Explain why each item belongs to its category...`

          // DON'T forget to request JSON format
          const prompt = `Extract items...` // May return prose